---
title: "Reproducibility Report for Study 4 and 5 by Boyce, Levy, and Futrell (2019, Journal of Memory and Language)"
author: "Jaylen Pittman (jaylen@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: 
      collapsed: true
---

<!-- Reproducibility reports should all use this template to standardize reporting across projects. These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

The authors find that using an Automated version of the Maze task (A-Maze) by generating distractor words with LSTM language models results in greater statistical power and localization of incremental processing effects when compared to Self-Paced Reading and Maze tasks with manually coded distractors.

### Justification for choice of study

Boyce and colleagues provide compelling evidence for the use of A-Maze in psycholinguistic experiements involving incremental processing, such as reading text on a screen. My own research investigates incremental reading processes and the effects of stereotyped language on processing outcomes via the same measures that Boyce and colleagues investigate, which is `surprisal`, or the negative log probability of a word `w` given a preceding context  (w<sub>1</sub>...w<sub>i</sub> ). I want to replicate the results of the A-Maze experiments because I plan to employ A-Maze in my own experiments and want to acquaint myself with the analytic pipeline. Along the way I can assess the reliability of the results in the experiments. 

### Anticipated challenges

Mostly, I think it will take some time to train the language models on a local machine (I trained one of the models before and it took a while). I also wonder how consistent the distractor generation will be given different machines, etc. I expect the LSTMs to be a little finicky. I also expect that it will take some time to get fully up and running with Ibex, since I am a little rusty there. However, the authors seem to have done a great job documenting their processes/pre-registrations/etc. so I think I am fortunate in that regard.

### Links

Project repository (on Github): <https://github.com/psych251/boycefutrelllevy2020/tree/main>
Original paper (as hosted in your repo): 
<https://github.com/psych251/boycefutrelllevy2020/tree/main/original_paper>

## Methods

### Description of the steps required to reproduce the results

Once the materials are prepared, the authors use a custom (now public) Ibex module for the maze task. Participants use the `e` and `i` keys to select among two words, and if the correct word is selected the experiment continues to the next word pair in the sentence. If the distractor word is selected the message “Incorrect! Press any key to continue” appears and the experiment continues to a new sentence where the participant can assess new word pairs. The authors also added a counter of words correct at the top of the screen; this counter only resets between experimental blocks. When the experiment finishes, the results are transmitted to the server and are recorded for download.

The authors make their materials for stimuli, set-up, distractor-selection, and online experiments publicly available via this link <https://vboyce.github.io/Maze/install.html>, so in theory as long as I follow these instructions and obtain a sufficiently powered sample I should be able to replicate their results.

### Differences from original study

There are two major differences between my study and the orginal study. The first involves the motivation of the studies. Boyce, Futrell, and Levy replicate non-automated variants of the Maze task (and Self-Paced Reading) to compare them with A-Maze to show the superiority of A-Maze for online experiments. I instead am replicating the automated portions only as a reliability metric and familiarization exercise. Second, the original authors use Amazon Mechanical Turk as their crowdsourcing platform, whereas I am using Prolific Academic. This impacts the participant selection process since Prolific uses demographic pre-screening for participants while M-Turk does not and instead relies on demographic survey questions.

Update: Originally, I planned for a replication of studies 4 and 5. Now, given that I perform a computational reproducibility check while the details of my server infrastructure are worked out to run the true replication, there are no differences in the study design; the main differenecs at the level of implementation are regarding potential hardware differences. 
(I ran the reproducibility check on a 2015 Macbook Pro with Retina Display (13-inch), using macOS Big Sur. The laptop has a 3.1 GHz Dual-Core Intel Core i7 Processor with a 16GB memory card and an Intel Iris Graphics 6100 1536 MB graphics card.)

## Project Progress Check 1

### Measure of success

The outcome measure of success is two-fold: 1) the code runs without any modifications (excepting package installs and directory changes); 2) The graph in the Key Analysis section is faithfully reproduced after the models are trained.


### Pipeline progress

On the computational reproducibility side, the pipeline of this set of experiments consists of loading the packages, loading the data, cleaning the data with the written functions provided by the authors of the original study, and running linear models with `brms` as discussed in the Key Analysis section.


## Results

### Data preparation

#### Materials Production
The materials production involves two steps, which the authors call "set-up stage" and "distractor-selection stage". During the set-up stage, the authors create look-up tables that map from words to frequencies and from `{length, frequency}` pairs to lists of potential distractor words (page 9). The Google Books NGrams corpus is the source of word frequencies. Distractor words are restricted to words in their dictionary that consisted entirely of lowercase letters, to avoid proper nouns; additionally, for the words that are entirely lowercase, they are able to be recapitalized to match the capitalization of the contending correct word that they are paired with.

In the distractor-selection stage, each sentence in a set of materials is iterated over at the sentence and word level; for each word in each sentence, a language model generates a distractor word matched for the correct word's length and approximate frequency while also controlling for contextual probability (distractor words should be have a low probability in the given sentential context).  

NB: The sentential stimuli are from Witzel et al (2012), which examine three different syntactic attachment preferences. 

The following flowchart shows the materials generation process.

![](/Users/jaylen/Documents/psych251/boycefutrelllevy2020/figures/Figure2BoyceFutrellLevy.png)
	
#### Participants 
The study was described as for native English speaking US citizens, but the authors mentioned that anyone could complete the experiment and get paid. The authors attempt to make up for this by including a demographics section to their experiment, where they asked three yes/no questions regarding US citizenship, native English speaker status, and current country of residence. Only participants who indicated that they were native English-speaking US citizens currently residing in the US were included in the analysis (no one was excluded from the study during the experiment on the basis of their answers). As mentioned above, this is a departure from my implementation of the experiment, as Prolific Academic utilizes pre-screeners to identify eligible participants during the participant recruitment process; thus in theory, no exclusions based on these question are needed (and so neither are the questions). 

#### Data Exclusions
The authors only include data from RTs where the correct word was chosen, since a participant’s choice of an incorrect word leads to the discarding of a sentence. Words for which the recorded RT were zero were excluded as this is indicative of a software error (one word was removed in their study for the A-Maze).

*Update from original replication plan*: all demographic exclusions are following in data processing pipeline specified by the original authors. See Participants section above.

#### Error analysis
Since the model has to create distractor words matched for length and frequency, there is substantial data loss at the beginning of sentences, where the most common words are articles and prepositions such as *the* and *of*. The authors report substantial data loss at word position two in their sentences: 
![Error rate by word position in manual distractor tasks (G-Maze) versus automated distractor tasks (A-Maze). Gulordava LSTM and Jozefowicz LSTM in purple and pink, respectively](/Users/jaylen/Documents/psych251/boycefutrelllevy2020/figures/Figure5BoyceFutrellLevy.png)

The data pipeline is fairly large and so is included as separate file in the class repository.

### Key analysis

The key analysis is the difference in RT between two conditions at the critical word position and at the following (spillover) region. The authors measure word positions from `+5`  (five words after critical word at `position = 0`) to `-5` (five words before critical word at `position = 0`. The authors use the following mixed effects model:

`log(RT) ~ condition + (condition | subject) + (condition | item)`


They do not adjust for multiple corrections, and RTs are averaged for two-word critical regions (e.g. "last night") and analyze them as one word.

Analysis was done in `R` using the `brms` package. Here are the main results for regions 0 - 3. I am interested in the results from the A-Maze experiments at the bottom (purple and pink lines, respectively).

```{r echo=FALSE}
knitr::include_graphics("/Users/jaylen/Documents/psych251/boycefutrelllevy2020/figures/mainfinding_original.png")
```


```{r echo=FALSE}
knitr::include_graphics("/Users/jaylen/Documents/psych251/boycefutrelllevy2020/figures/mainfinding_reproduced.png")
```

## Discussion

### Summary of Reproduction Attempt

Given the criteria for successful reproduction attempts that I laid out earlier in this document, this reproduction was successful. I was able to easily run the code with no non-nominal modifications (directories/package installs). Additionally, I am able to replicate the initial results of interest (studies 4 and 5) as well as the findings for the Lab and Web versions of the Maze task on almost all counts -- the significant difference at word position 1 in the Web L-Maze of the original study (p = 0.047) as now become non-significant, which is unsurprising due to that value's proximity to the p = 0.05 critical value used in this paper and the fact that `brms` uses Monte-Carlo simulation in model selection.

### Commentary

Overall the data-sharing and data management practices implemented by the authors of this study were exemplary, and made a computational reproducibility study straightforward. In terms of replication, however, the paper's emphasis on easy implementation with Ibex Farm was unfortunately undermined by the Ibex Farm's shutdown in September 2021. Notwithstanding this hiccup, replications are currently in progress after a few initial infrastructural problems (e.g. server set-up/web-hosting on compatible servers).
